{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "from typing import Dict\n",
    "import html\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pre processing\n",
    "## keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "## nltk\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = 'data/1_raw/'\n",
    "TOOLS_PATH = 'data/tools/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T12:44:58.792675Z",
     "iopub.status.busy": "2022-10-06T12:44:58.792197Z",
     "iopub.status.idle": "2022-10-06T12:44:58.798319Z",
     "shell.execute_reply": "2022-10-06T12:44:58.796971Z",
     "shell.execute_reply.started": "2022-10-06T12:44:58.792640Z"
    }
   },
   "source": [
    "## Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = [\"label\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "\n",
    "df = pd.read_csv(RAW_PATH + 'sentiment140/training.1600000.processed.noemoticon.csv',\n",
    "                 encoding =DATASET_ENCODING,\n",
    "                 names=DATASET_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = pd.read_csv(TOOLS_PATH + 'contractions.csv',\n",
    "                           index_col='Contraction')\n",
    "contractions.index = contractions.index.str.lower()\n",
    "contractions.Meaning = contractions.Meaning.str.lower()\n",
    "contractions_dict = contractions.to_dict()['Meaning']\n",
    "del contractions_dict['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "#           ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "#           ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "#           ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "#           '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "#           '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "#           ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "with open(TOOLS_PATH + 'emojis.json', 'r') as file:\n",
    "    emojis = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1600000\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label          id                          date      flag             user  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                text  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1042015</th>\n",
       "      <td>1</td>\n",
       "      <td>1957148701</td>\n",
       "      <td>Thu May 28 23:39:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kassymay</td>\n",
       "      <td>it's friday  gonna go force myself to study ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597803</th>\n",
       "      <td>1</td>\n",
       "      <td>2193029327</td>\n",
       "      <td>Tue Jun 16 07:53:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>allora</td>\n",
       "      <td>@jumpsun you'll rarely see me without a smile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139891</th>\n",
       "      <td>0</td>\n",
       "      <td>1880908014</td>\n",
       "      <td>Fri May 22 02:55:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AlexaGrace16</td>\n",
       "      <td>@birdb  went to bed early missed u on msn   ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98997</th>\n",
       "      <td>0</td>\n",
       "      <td>1793477150</td>\n",
       "      <td>Thu May 14 02:50:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>WiseTC</td>\n",
       "      <td>i have to catch a bus today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364463</th>\n",
       "      <td>0</td>\n",
       "      <td>2048169043</td>\n",
       "      <td>Fri Jun 05 14:46:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>UKSolarCar</td>\n",
       "      <td>So we had a flat tire, but we were able to dri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label          id                          date      flag  \\\n",
       "1042015      1  1957148701  Thu May 28 23:39:18 PDT 2009  NO_QUERY   \n",
       "1597803      1  2193029327  Tue Jun 16 07:53:56 PDT 2009  NO_QUERY   \n",
       "139891       0  1880908014  Fri May 22 02:55:56 PDT 2009  NO_QUERY   \n",
       "98997        0  1793477150  Thu May 14 02:50:43 PDT 2009  NO_QUERY   \n",
       "364463       0  2048169043  Fri Jun 05 14:46:13 PDT 2009  NO_QUERY   \n",
       "\n",
       "                 user                                               text  \n",
       "1042015      kassymay  it's friday  gonna go force myself to study ho...  \n",
       "1597803        allora  @jumpsun you'll rarely see me without a smile ...  \n",
       "139891   AlexaGrace16  @birdb  went to bed early missed u on msn   ta...  \n",
       "98997          WiseTC                       i have to catch a bus today   \n",
       "364463     UKSolarCar  So we had a flat tire, but we were able to dri...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Traitements simples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Doublons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a vu qu'il y a 1685 tweets r√©p√©t√©s deux fois avec une version avec le label 0, et une version avec le label 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(subset=['id'], keep=False)]\n",
    "\n",
    "# Filtrer le DataFrame pour exclure les doublons\n",
    "df = df[~df['id'].isin(duplicates['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1596630, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Selection des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = df[['label','text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Echantillonnage √©quilibr√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_sample_df(df, labels, size, random_state=None):\n",
    "    new_df = pd.DataFrame()\n",
    "    for label in labels:\n",
    "        new_df = pd.concat([new_df, df[df.label==label].sample(size//len(labels), random_state=random_state)])\n",
    "    return new_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE_SIZE = 1600\n",
    "# tweets_df = balanced_sample_df(tweets_df, [0, 1], SAMPLE_SIZE, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pr√©paration des donn√©es textuelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1. Substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue dans un premier temps un certain nombre de subsitutions :\n",
    "\n",
    "1. **Remplacement des URLs:** les liens d√©butant par **'http' or 'https' or 'www'** sont remplac√©s par **'<url\\>'**.\n",
    "2. **Remplacement des utilisateurs:** on remplace les @Usernamespar le mot **'<user\\>'**. ['@Kaggle' to '<user\\>'].\n",
    "3. **Remplacement des lettres cons√©cutives:** 3 or more consecutive letters are replaced by 2 letters. ['Heyyyy' to 'Heyy']\n",
    "4. **Remplacement des Emojis:** on remplace les emojis par leur sens. [':)' to '<smile\\>']\n",
    "5. **Remplacement des Contractions:**: on remplace les contractions par leur forme d√©velopp√©e. [\"can't\" to 'can not']\n",
    "6. **Remplacement des caract√®res sp√©ciaux:** on remplace les caract√®res qui ne sont pas des chiffres, lettres, caract√®res pr√©d√©finis par un espace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitution(text: str, replacements: Dict[str, str]) -> str:\n",
    "    text = text.lower()\n",
    "    for contraction, replacement in replacements.items():\n",
    "        text = text.replace(contraction, replacement)   \n",
    "    return text\n",
    "\n",
    "def replace_html_entities(text: str) -> str:\n",
    "    return html.unescape(text)\n",
    "\n",
    "def substitute_url(text: str, replacement: str = 'url') -> str:\n",
    "    text = re.sub(r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\", replacement, text)\n",
    "    return text\n",
    "    \n",
    "def substitute_user(text: str, replacement: str = 'user') -> str:\n",
    "    text = re.sub('@[^\\s]+', replacement, text)\n",
    "    return text\n",
    "\n",
    "def filter_non_alphabet(text: str) -> str:\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    return text\n",
    "    \n",
    "def replace_three_same_letters(text: str) -> str:\n",
    "    text = re.sub(r\"(.)\\1\\1+\", r\"\\1\\1\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_text(text):\n",
    "    text = text.lower()\n",
    "    text = replace_html_entities(text)\n",
    "    text = substitution(text, contractions_dict)\n",
    "    text = substitution(text, emojis)\n",
    "    text = substitute_url(text)\n",
    "    text = substitute_user(text)\n",
    "    text = filter_non_alphabet(text)\n",
    "    text = replace_three_same_letters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example & text with <HTML> entities.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_with_entities = \"This is an example &amp; text with &lt;HTML&gt; entities.\"\n",
    "processed_text = replace_html_entities(text_with_entities)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing steps for tweet id=623132\n",
      "\n",
      "0. i've been conscripted to a work politics  war i don't want to fight in. why can't i be switzerland  \n",
      "\n",
      "1 i have been conscripted to a work politics  war i do not want to fight in. why cannot i be switzerland  \n",
      "\n",
      "2. i have been conscripted to a work politics  war i do not want to fight in. why cannot i be switzerland  \n",
      "\n",
      "3. i have been conscripted to a work politics  war i do not want to fight in. why cannot i be switzerland  \n",
      "\n",
      "4. i have been conscripted to a work politics  war i do not want to fight in. why cannot i be switzerland  \n",
      "\n",
      "5. i have been conscripted to a work politics  war i do not want to fight in. why cannot i be switzerland  \n",
      "\n",
      "6. i have been conscripted to a work politics  war i do not want to fight in  why cannot i be switzerland  \n",
      "\n",
      "7. i have been conscripted to a work politics  war i do not want to fight in  why cannot i be switzerland  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = random.randint(0, len(tweets_df.text))\n",
    "print(f\"Preprocessing steps for tweet id={k}\\n\")\n",
    "tweet = tweets_df.text.iloc[k].lower()\n",
    "\n",
    "print('0.', tweet, \"\\n\")\n",
    "\n",
    "tweet = substitution(tweet, contractions_dict)\n",
    "print('1', tweet, \"\\n\")\n",
    "\n",
    "tweet = replace_html_entities(tweet)\n",
    "print('2.', tweet, \"\\n\")\n",
    "\n",
    "tweet = substitution(tweet, emojis)\n",
    "print('3.', tweet, \"\\n\")\n",
    "\n",
    "tweet = substitute_url(tweet)\n",
    "print('4.', tweet, \"\\n\")\n",
    "\n",
    "tweet = substitute_user(tweet)\n",
    "print('5.', tweet, \"\\n\")\n",
    "\n",
    "tweet = filter_non_alphabet(tweet)\n",
    "print('6.', tweet, \"\\n\")\n",
    "\n",
    "tweet = replace_three_same_letters(tweet)\n",
    "print('7.', tweet, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'man my signal sux downstairs  on my way back upstairs  brb '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substitute_text(tweets_df.text[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application sur les donn√©es**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_substituted = [substitute_text(tweet) for tweet in tweets_df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user url  aww  that is a bummer  you shoulda got david carr of third day to do it  wink',\n",
       " 'is upset that he cannot update his facebook by texting it  and might cry as a result  school today also  blah ',\n",
       " 'user i dived many times for the ball  managed to save 50  the rest go out of bounds',\n",
       " 'my whole body feels itchy and like its on fire ',\n",
       " 'user no  it is not behaving at all  i am mad  why am i here  because i cannot see you all over there  ',\n",
       " 'user not the whole crew ',\n",
       " 'need a hug ',\n",
       " 'user hey  long time no see  yes  rains a bit  only a bit  lol  i am fine thanks  how is you  ',\n",
       " 'user nope they did not have it ',\n",
       " 'user que me muera  ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_substituted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, filtered_words, stem_or_lem=''):\n",
    "    raw_tokens_list = word_tokenize(text)\n",
    "    tokens_list = []\n",
    "    if stem_or_lem in [\"stem\", 'lem']:\n",
    "        for token in raw_tokens_list:\n",
    "            if token in filtered_words:\n",
    "                continue\n",
    "            if stem_or_lem == \"stem\":\n",
    "                stemmer = PorterStemmer()\n",
    "                tokens_list.append(stemmer.stem(token))\n",
    "            else:\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                tokens_list.append(lemmatizer.lemmatize(token))\n",
    "    else:\n",
    "        tokens_list = raw_tokens_list\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 47s\n",
      "Wall time: 2min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets_df['tokens'] = [tokenize_text(tweet, filtered_words=filtered_words) for tweet in text_substituted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 28s\n",
      "Wall time: 2min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets_df['tokens_lem'] = [tokenize_text(tweet, filtered_words=filtered_words) for tweet in text_substituted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8min 47s\n",
      "Wall time: 8min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets_df['tokens_stem'] = [tokenize_text(tweet, filtered_words=filtered_words, stem_or_lem='stem') for tweet in text_substituted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour des gains de place, on exporte les donn√©es sous forme de texte plutot que de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['clean'] = tweets_df['tokens'].apply(lambda x: \" \".join(x))\n",
    "tweets_df['clean_lem'] = tweets_df['tokens_lem'].apply(lambda x: \" \".join(x))\n",
    "tweets_df['clean_stem'] = tweets_df['tokens_stem'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.getcwd()[2:]\n",
    "PROJECT_PATH = PROJECT_PATH.replace(\"\\\\\", \"/\") + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/USER/PycharmProjects/2_OC_IA/sentiment_analysis/src/data/2_preprocessed/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/2_preprocessed/'\n",
    "full_path = urljoin(PROJECT_PATH, DATA_PATH)\n",
    "full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(full_path):\n",
    "    os.makedirs(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_file = f'tweets_tokens_df_{str(tweets_df.shape[0])}.csv'\n",
    "# df_file\n",
    "\n",
    "# tweets_df[['label', 'text', 'tokens', 'tokens_lem', 'tokens_stem']].to_csv(urljoin(full_path, df_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file = f'tweets_join_df_{str(tweets_df.shape[0])}.csv'\n",
    "df_file\n",
    "\n",
    "tweets_df[['label', 'text', 'clean', 'clean_lem', 'clean_stem']].to_csv(urljoin(full_path, df_file))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2477,
     "sourceId": 4140,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2567255,
     "sourceId": 4364581,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2614036,
     "sourceId": 4610952,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30260,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
